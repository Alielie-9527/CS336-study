# 大模型量化与混合精度详解

## 1.1 核心概念辨析

为了厘清容易混淆的概念，我们需要区分“混合精度训练”与“大模型量化”的应用场景与目的。

| 概念                          | 场景      | 数据格式    | 核心目的                                                     |
| :---------------------------- | :-------- | :---------- | :----------------------------------------------------------- |
| **混合精度训练 (AMP)**        | 训练/微调 | FP16 + FP32 | 在保持精度的前提下，减少显存占用，加速**训练**过程。         |
| **大模型量化 (Quantization)** | 推理/部署 | INT8 / INT4 | 降低参数精度，大幅减少显存占用，提高**推理**速度，使其能在消费级硬件上运行。 |

> **注**：量化通常指的是将高精度浮点数（FP16/FP32）映射为低精度整数（INT8/INT4）。

---

## 1.2 量化技术流派

量化技术主要分为两大流派，目前普通用户和开发者接触最多的是 PTQ。

### 1. PTQ (Post-Training Quantization, 训练后量化)

*   **定义**：模型训练完成后，直接对权重/激活值进行压缩。
*   **特点**：无需重新训练，计算资源消耗低。
*   **代表技术**：**GPTQ**, **AWQ**, **GGUF**, **LLM.int8()**。

### 2. QAT (Quantization-Aware Training, 量化感知训练)

*   **定义**：在训练过程中模拟量化操作，让模型在训练时就“适应”低精度带来的误差。
*   **特点**：精度损失最小，但训练难度大、成本极高。

---

## 1.3 量化的数学原理

量化的本质是建立浮点数（$r$）与整数（$q$）之间的映射关系。

### 1. 对称量化 (Symmetric Quantization)

通常用于 **AbsMax 量化**。这种方法计算简单，但对非对称分布的数据（如 ReLU 激活后的数据）可能造成范围浪费。

*   **步骤 1：寻找绝对值最大值**
    $$ \text{absmax} = \max(|X|) $$
*   **步骤 2：计算缩放因子 (Scale)**
    由于 INT8 的表示范围为 $[-127, 127]$：
    $$ S = \frac{\text{absmax}}{127} $$
*   **步骤 3：量化映射**
    $$ X_{\text{int8}} = \text{round}\left( \text{clamp}\left( \frac{X_{\text{fp32}}}{S}, -127, 127 \right) \right) $$
*   **步骤 4：反量化 (Dequantization)**
    $$ X_{\text{fp32}} \approx X_{\text{int8}} \times S $$

### 2. 非对称量化 (Asymmetric / Zeropoint Quantization)

通过引入零点（Zero-point），解决数据分布不对称的问题（即数据的 0 点并不对应整数的 0 点）。

假设浮点数范围为 $[r_{\min}, r_{\max}]$，目标整数范围为 $[q_{\min}, q_{\max}]$。映射公式为线性变换：
$$ q = \text{round}\left( \frac{r}{S} + Z \right) $$

*   **(1) 缩放系数 (Scale):**
    $$ S = \frac{r_{\max} - r_{\min}}{q_{\max} - q_{\min}} $$
*   **(2) 零点 (Zero-point):**
    $$ Z = \text{round}\left( q_{\min} - \frac{r_{\min}}{S} \right) $$
*   **(3) 反量化过程:**
    $$ r \approx S \times (q - Z) $$

### 3.百分位数量化（都是基于上述原理）

为了减轻上述两种算法对离群值的敏感性，在校准期间收集数值的分布，然后确定上下限的百分数阈值，比如0.1%和99.9%,那么在这个范围外的数都会变为最大值或最小值。

**优点：**

- 比 MinMax 对异常值更具抵抗力。
- 实现相对简单，只需收集直方图和计算百分位数。

**缺点：**

- 对被截断的异常值引入饱和误差。如果这些异常值带有重要信息，截断它们可能会对精度产生负面影响。
- 需要选择合适的百分位数，这成为一个需要调整的超参数。最佳百分位数可能因不同层或模型而异。



### 4.熵（KL散度）量化

采用更科学地方式选择这个截断范围，**KL散度：**平衡两个分布（P和Q）之间差异度。希望让截断后量化地数据分布和原始的数据分布最像

**具体步骤：**

1.  **准备**：统计原始数据分布 $P$（FP32）。
2.  **遍历测试**：假设我们尝试很多个不同的截断阈值（比如 $T=1, T=2, ..., T=100$）。
3.  **模拟量化**：对于每一个 $T$：
    *   把数据截断到 $[-T, T]$ 并量化。
    *   然后再反量化回来，得到分布 $Q$。
4.  **对比**：计算 $P$ 和 $Q$ 之间的 KL 散度。
5.  **决策**：找到那个**KL 散度最小**的 $T$，这就是最佳截断阈值。



*   **优点**：
    *   **抗干扰**
    *   **精度高**：因为范围变小了（比如从 $[-1, 1000]$ 变成了 $[-5, 5]$），每一格（Step）代表的数值更精细了，主要数据被照顾得更好。
*   **缺点**：
    *   **信息丢失**：被切掉的那 $0.1\%$ 数据如果包含重要信息（比如某些特殊的激活信号），模型可能会变傻。
    *   **调参麻烦**：选 $99.9\%$ 还是 $99\%$？不同模型、不同层可能不一样，需要试。

## 2. 算法选择指南

根据数据的**分布特征**来决定：

| 目标对象                 | 数据特征                                   | 推荐算法                        | 原因                                                         |
| :----------------------- | :----------------------------------------- | :------------------------------ | :----------------------------------------------------------- |
| **权重 (Weights)**       | **稳定、对称**(通常像正态分布)             | **MinMax** 或 **百分位数**      | 权重是训练好的，通常没有极端离群值，且分布比较“漂亮”，简单方法就够用。 |
| **激活值 (Activations)** | **不稳定、非对称、长尾**(可能有巨大离群值) | **熵量化 (KL)** 或 **百分位数** | 激活值随输入变化，且经常有怪异的极值（如 ReLU 后的长尾）。MinMax 会导致精度崩塌，必须用截断策略。 |

### 总结对比表

| 算法         | 核心逻辑             | 速度 | 对离群值抵抗力  | 精度上限        |
| :----------- | :------------------- | :--- | :-------------- | :-------------- |
| **MinMax**   | 以此为界，全都要     | 最快 | 极差 (被带跑偏) | 低 (易受干扰)   |
| **百分位数** | 切掉头尾，保留中间   | 中等 | 强 (强制切除)   | 中高 (依赖调参) |
| **熵 (KL)**  | 算出来的“最像”截断点 | 最慢 | 强 (智能平衡)   | **最高**        |

---

## 3. LLM.int8() 技术详解

**LLM.int8()** 本质上属于 **PTQ（在线动态量化）**。它的核心贡献在于解决了大模型中“离群值”导致量化精度崩塌的问题。

### 核心思想：混合精度分解 (Mixed Precision Decomposition)

大模型中绝大部分数值分布集中，适合量化；但极少数特征维度存在数值巨大的**系统性离群值 (Outliers)**。如果强行统一量化，这些离群值会拉大 Range，导致正常值的量化分辨率极低。

**解决方案**：根据阈值 $\alpha$ 将矩阵分解为两部分，分别计算。

1. **分解矩阵**：
   $$ X = X_{\text{regular}} + X_{\text{outlier}} $$
   $$ W = W_{\text{regular}} + W_{\text{outlier}} $$

   > 注：这里 $X_{\text{outlier}}$ 仅保留超过阈值的列，其余位置补 0；$X_{\text{regular}}$ 则保留其余部分。

2. **分流计算**：

   *   **离群部分 ($X_{\text{outlier}}$)**：保持 **FP16** 高精度计算，确保不损失精度。
   *   **正常部分 ($X_{\text{regular}}$)**：转换为 **INT8** 进行向量化计算，提高效率。

3. **合并结果**：
   由于分解是基于维度的（类似于掩码操作），交叉项乘积为 0，因此：
   $$ \text{Result} \approx (X_{\text{regular}} \times W_{\text{regular}})_{\text{INT8}} + (X_{\text{outlier}} \times W_{\text{outlier}})_{\text{FP16}} $$





## 4. 补充：LLM.int8() 的具体动态量化与推理流程

LLM.int8() 的核心魔法在于**推理时（Inference-time）**的动态处理。它不仅仅是将模型存为 8-bit，更关键的是在计算矩阵乘法（$Y = X \times W$）时，如何实时处理数据。

### 1. 前置概念：逐向量量化 (Vector-wise Quantization)

传统的量化可能整个矩阵共用一个缩放因子 $S$（Per-tensor），这会导致精度损失。LLM.int8() 采用更精细的策略：

*   **对于权重 $W$**：每一**列**都有独立的缩放常数。
*   **对于输入（激活值）$X$**：每一**行**（即每个 Token）都有独立的缩放常数。
    这保证了量化的精度基础。

### 2. 详细推理流程 (Step-by-Step)

假设我们正在进行一层 Transformer 的计算，输入是隐藏层状态 $X$（FP16），权重是 $W$（已存储为 INT8）。

#### 第一步：离群值检测 (Outlier Detection)

当输入数据 $X$（FP16）到来时，系统会扫描 $X$ 的每一个元素。

* **规则**：检查每一列（特征维度）中是否存在绝对值超过阈值（通常为 **6.0**）的元素。

* **结果**：找到那些包含“剧烈波动”数值的特征列索引，记为 $I_{\text{outlier}}$。

  > **为什么叫动态？** 因为 $X$ 是用户输入产生的，每一轮对话的 $X$ 都不一样，所以离群值的列也是实时变化的。

#### 第二步：混合精度分解 (Decomposition)

根据上一步找到的索引 $I_{\text{outlier}}$，将矩阵 $X$ 和权重矩阵 $W$ 实时“撕”成两半：

1.  **离群部分 (FP16 通道)**：
    *   从 $X$ 中提取出这些列，形成 $X_{\text{fp16}}$。
    *   从 $W$ 中提取出对应的行（反量化为 FP16），形成 $W_{\text{fp16}}$。
    *   *注：这部分数据量极小（通常 < 0.1%），但对精度至关重要。*

2.  **正常部分 (INT8 通道)**：
    *   $X$ 中剩下的列是平稳的，将其进行 **动态量化**：
        1.  计算每一行的绝对最大值。
        2.  计算缩放因子 $S_x$。
        3.  映射为 INT8，形成 $X_{\text{int8}}$。
    *   $W$ 中剩下的行本身就是 INT8 存储的，直接取出作为 $W_{\text{int8}}$。

#### 第三步：分流计算 (Matrix Multiplication)

现在我们并行执行两次矩阵乘法：

1.  **高精度流**：
    $$ O_{\text{outlier}} = X_{\text{fp16}} \times W_{\text{fp16}} $$
    *(结果是 FP16)*

2.  **低精度流**：
    $$ O_{\text{regular}} = X_{\text{int8}} \times W_{\text{int8}} $$
    *(结果是 INT32)*
    *   **反量化**：计算完成后，立刻利用权重和输入的缩放因子 ($S_w, S_x$) 将结果转回 FP16。
        $$ O_{\text{regular\_fp16}} = O_{\text{regular}} \times S_w \times S_x $$

#### 第四步：结果合并 (Merge)

将两部分结果相加，得到最终输出：
$$ Y_{\text{output}} = O_{\text{outlier}} + O_{\text{regular\_fp16}} $$

### 3. 总结：为什么它慢但效果好？

*   **效果好**：因为它不像普通量化那样强制把 100 和 0.1 压在同一个 8-bit 空间里。它“尊重”了那 0.1% 的捣乱分子（离群值），给它们开了 VIP 通道（FP16）。
*   **速度慢**：因为每次推理都要**实时**扫描 $X$、**实时**拆分矩阵、**实时**反量化部分权重。这比直接把整个矩阵扔进 GPU 算要繁琐得多。

---

## 5 .训练后量化 (PTQ) 流程与分类

### 1. PTQ 的对象分类

*   **Weight-Only (仅权重量化)**
    *   **含义**：只压缩模型参数，激活值计算时仍用 FP16。
    *   **优缺**：精度损失极小，显存节省明显。
    *   **代表**：**GPTQ**, **AWQ**, **GGUF (k-quants)**。*(目前最主流)*
*   **Weight + Activation (全量化)**
    *   **含义**：参数和激活值都量化。
    *   **优缺**：利用 INT8 Tensor Core 速度最快，但精度极易受损，需配合 SmoothQuant 等技术处理离群值。

### 2. 标准 PTQ 实施流程

PTQ 的过程实际上就是寻找最佳 $S$ 和 $Z$ 参数并应用的过程。

1.  **准备模型**：加载预训练的全精度模型（如 FP16/FP32）。
2.  **数据校准 (Calibration)**：
    *   准备一个小型的、有代表性的数据集（校准集）。
    *   让模型在该数据集上运行推理。
3.  **统计范围 (Observer)**：
    *   收集每一层的权重和**激活值**的统计数据（主要是 $r_{\min}$ 和 $r_{\max}$）。
4.  **计算参数**（静态体现）：
    *   根据统计数据，计算最佳的 **Scale ($S$)** 和 **Zero-point ($Z$)**。（这样在推理过程中就不需要生成而是使用先验的。
    *   *注：GPTQ/AWQ 等算法在此步骤有更复杂的优化策略（如最小化重构误差）。*
5.  **实施量化**：
    *   利用 $S$ 和 $Z$ 将 FP32 权重转换为 INT8/INT4。
6.  **保存模型**：
    *   存储量化后的权重以及对应的量化参数（$S, Z$），供推理引擎在运行时进行反量化或直接计算。

### 3.静态量化与动态量化

#### 静态量化

静态量化是在在执行推理*之前*对模型权重和激活进行量化。由于激活的取值范围可能因输入数据而异，静态量化需要一个校准步骤

**优点：**

- 更高的潜在性能： 由于所有量化参数都预先固定，推理可以在支持的硬件（CPU、GPU、加速器）上完全使用高度优化的整数算术指令运行，从而实现最大程度的加速和能效。
- 更低的推理开销： 计算激活范围或量化参数无需运行时成本

**缺点：**

- 需要校准数据集
- 对离群值敏感

**注意这里LLM主流的Weight-Only 静态量化（W4A16/W8A16）和一般的不一样：** 对输入不进行量化，对权重进行反量化



#### 动态量化

动态量化，在某些情况下也常被称为“仅权重量化”，采取了不同的方法。权重在离线阶段量化，这一点与静态量化类似。然而，激活是在推理过程中*动态*或*即时*量化的。

1. **离线权重量化：** 模型权重被转换为低精度整数格式（例如，INT8*I**NT*8）并存储。
2. **推理：**
   - 当输入数据到来时，激活会逐层处理。
   - **对于涉及量化权重的操作（如矩阵乘法），传入的浮点激活会在操作前立即量化**。它们的范围（最小值/最大值）是根据当前批次数据动态计算的。
   - 计算（例如，`INT8`权重 * `INT8`激活）得以执行。
   - 结果通常在传递给需要浮点输入的下一个操作或层（如某些激活函数或归一化层）之前，反量化回浮点数（FP32*FP*32）。



------



# 量化推理中的反量化与噪声分析

## 1. 核心结论

*   **误区澄清**：反量化（Dequantization）本身**不产生噪声**。
*   **真实情况**：噪声（误差）是在**量化（FP16 $\to$ INT）**那一刻产生的（有损压缩）。反量化只是将“低精度整数”还原为“高精度浮点数”的格式转换（$r = S \times q + Z$），它只是“显露”了误差，而非“增加”误差。

## 2. 现代推理流程 (以 W4A16 为例)

现代 LLM 推理采用**“低精度存储，高精度计算”**的策略，流程如下：

1.  **加载 (Load)**：从显存读取 INT4 权重（**节省显存**）。
2.  **反量化 (Dequant)**：在计算单元（Tensor Core）附近，瞬间将 INT4 恢复为 FP16（**恢复格式**）。
3.  **计算 (Compute)**：使用 FP16 进行矩阵乘法（**高精度环境**）。
    *   $$Y_{\text{fp16}} = X_{\text{fp16}} \times \text{Dequant}(W_{\text{int4}})$$

> **关键点**：中间结果（Activations）始终保持 FP16 流动，避免了“每一层都重新量化”带来的二次噪声引入。

## 3. 为什么误差不会层层累积导致崩溃？

虽然初始量化有误差，但通过以下机制抑制了传播：

### A. 算法层面的补偿 (Calibration)

*   **技术**：GPTQ, AWQ。
*   **原理**：在量化前，通过数学方法调整未量化的参数，使得“量化后的输出”在数学期望上无限接近“原版 FP16 的输出”。误差被算法“预消化”了。

### B. 物理层面的隔离 (Outlier Protection)

*   **技术**：LLM.int8(), AWQ。
*   **原理**：对于极少数容易放大噪声的**离群值**（Outliers），**不进行量化**，直接使用 FP16 传输和计算。

### C. 计算层面的缓冲 (High Precision Compute)

*   **原理**：利用 FP16（甚至 FP32 累加器）的高精度计算环境，来消化低精度权重带来的微小扰动。

