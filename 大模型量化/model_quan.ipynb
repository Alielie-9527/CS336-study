{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71049e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets optimum[onnxruntime] accelerate torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 加载模型 \n",
    "# 模型介绍：distilbert-base-uncased-finetuned-sst-2-english 是一个经过微调的 DistilBERT 模型，用于情感分析任务，能够将文本分类为正面或负面情绪。\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# print(\"模型加载完成\")\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6153d5",
   "metadata": {},
   "source": [
    "# 量化校准数据集准备\n",
    "\n",
    "由于该模型使用于情感分类的，所以我们选择一个情感分析的数据集中的小部分数据，用于在推理期间观察激活值的分布，从而进行量化校准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba077903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载完成\n",
      "{'idx': [0, 1, 2], 'sentence': ['hide new secretions from the parental units ', 'contains no wit , only labored gags ', 'that loves its characters and communicates something rather beautiful about human nature '], 'label': [0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# 加载数据集 \n",
    "dataset_name = \"sst2\" # 斯坦福的情感分类数据集 是个二分类数据集\n",
    "dataset = load_dataset(dataset_name, split=\"train[:5%]\")  # 只取训练集的前5%作为校准数据集\n",
    "print(\"数据集加载完成\")\n",
    "#  对数据格式不清楚可以查看数据集的前几条数据\n",
    "print(dataset[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d081b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理完成\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "# tokenizer 返回的是一个字典，包括 input_ids、attention_mask 等\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(\"数据预处理完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3521e5",
   "metadata": {},
   "source": [
    "# 使用Optimum配置量化参数(CPU量化)\n",
    "\n",
    "\n",
    "## 库函数介绍\n",
    "ORTModelForSequenceClassification 是一个专门用于序列分类任务的类，它封装了一个已经转换为 ONNX 格式的 Transformer 模型，并使用 ONNX Runtime 作为其推理引擎。\n",
    "\n",
    "ORTQuantizer 用于创建量化器，它可以将 ONNX 模型转换为量化后的版本，以提高推理效率和减少模型大小。量化器支持多种量化方法和配置选项，可以根据具体需求进行调整。\n",
    "\n",
    "AutoCalibrationDataLoader 是一个自动化的数据加载器，专门用于量化校准过程。它可以从给定的数据集中加载数据，并在推理期间观察激活值的分布，以便进行量化校准。该类简化了数据加载和预处理的过程，使得量化校准更加高效和便捷。\n",
    "\n",
    "AutoQuantizationConfig 是一个自动化的量化配置类，用于定义量化过程中的各种参数和选项。它支持多种量化方法（如动态量化、静态量化等）和配置选项，可以根据具体需求进行调整。该类简化了量化配置的过程，使得量化操作更加高效和便捷。\n",
    "## ONNX模型介绍\n",
    "- ONNX（Open Neural Network Exchange）是一种开放的深度学习模型交换格式，旨在促进不同深度学习框架之间的互操作性。通过将模型导出为 ONNX 格式，可以在不同的运行时环境中使用该模型，而无需依赖于特定的深度学习框架。相比之下pytorch模型的运行需要各种依赖各种库。\n",
    "- ONNX 模型文件通常以 `.onnx` 为扩展名，包含了模型的计算图、权重参数以及其他相关信息。ONNX 模型可以在支持 ONNX 格式的各种推理引擎中运行，如 ONNX Runtime、TensorRT 等。\n",
    "- 使用 ONNX 模型的主要优势包括跨框架兼容性、优化的推理性能以及简化的部署流程。通过将模型转换为 ONNX 格式，可以更容易地在不同的平台和设备上部署深度学习模型。\n",
    "- ONNX Runtime 是一个高性能的推理引擎，专门用于运行 ONNX 格式的深度学习模型。它支持多种硬件平台，包括 CPU、GPU 以及其他加速器，旨在提供快速且高效的模型推理能力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08d60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model distilbert-base-uncased-finetuned-sst-2-english was already converted to ONNX but got `export=True`, the model will be converted to ONNX once again. Don't forget to save the resulting model with `.save_pretrained()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始校准...\n",
      "Collecting tensor data and making histogram ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda/envs/HiChunk_w/lib/python3.10/site-packages/numpy/lib/histograms.py:353: RuntimeWarning: overflow encountered in subtract\n",
      "  return np.subtract(a, b, dtype=dt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal threshold for each tensor using 'percentile' algorithm ...\n",
      "Number of tensors : 173\n",
      "Number of histogram bins : 2048\n",
      "Percentile : (0.0049999999999954525,99.995)\n",
      "校准完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.embeddings.LayerNorm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.0.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.0.output_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.1.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.1.output_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.2.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.2.output_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.3.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.3.output_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.4.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.4.output_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.5.sa_layer_norm.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'distilbert.transformer.layer.5.output_layer_norm.weight' with rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化完成！模型已保存至: ./quantized_distilbert_sst2\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer, AutoQuantizationConfig, ORTModelForSequenceClassification\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# 1. 加载并导出为ONNX模型\n",
    "# 必须设置 export=True，因为原始模型是 PyTorch 格式，我们需要先将其转换为 ONNX\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_name, export=True)\n",
    "\n",
    "# 2. 定义量化配置\n",
    "# 使用 avx2 指令集优化，静态量化，逐通道量化\n",
    "qconfig = AutoQuantizationConfig.avx2(is_static=True, per_channel=True)\n",
    "\n",
    "# 3. 创建量化器\n",
    "quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
    "\n",
    "# 4. 准备校准数据集 (关键修改：减少数据量)\n",
    "# 崩溃原因：percentiles 方法需要记录所有激活值，数据量太大导致内存溢出 (OOM)\n",
    "# 解决方法：只使用 100-200 条数据进行校准通常就足够了\n",
    "columns_to_remove = [col for col in encoded_dataset.column_names if col not in tokenizer.model_input_names]\n",
    "calibration_dataset = encoded_dataset.remove_columns(columns_to_remove).shuffle(seed=42).select(range(128))\n",
    "\n",
    "# 5. 定义校准配置\n",
    "# 使用 percentiles 方法，但现在数据量小了，不会崩溃\n",
    "calibration_config = AutoCalibrationConfig.percentiles(calibration_dataset, percentile=99.995)\n",
    "\n",
    "# 6. 执行校准\n",
    "print(\"开始校准...\")\n",
    "ranges = quantizer.fit(\n",
    "    dataset=calibration_dataset,\n",
    "    calibration_config=calibration_config,\n",
    "    operators_to_quantize=qconfig.operators_to_quantize,\n",
    ")\n",
    "print(\"校准完成\")\n",
    "\n",
    "# 7. 执行量化并保存\n",
    "quantized_model_path = \"./quantized_distilbert_sst2\"\n",
    "quantizer.quantize(\n",
    "    save_dir=quantized_model_path,\n",
    "    quantization_config=qconfig,\n",
    "    calibration_tensors_range=ranges\n",
    ")\n",
    "print(f\"量化完成！模型已保存至: {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ad7149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模型大小对比 ===\n",
      "原始模型大小 (FP32 ONNX): 255.54 MB\n",
      "量化模型大小 (INT8 ONNX): 65.39 MB\n",
      "模型压缩率: 3.91倍\n",
      "\n",
      "=== 推理速度对比 (CPU, Batch Size=1) ===\n",
      "原始模型平均耗时: 21.11 ms\n",
      "量化模型平均耗时: 21.93 ms\n",
      "速度提升: 0.96倍\n",
      "注意：在 Batch Size=1 时，CPU 计算往往不是瓶颈，Python 调用开销占比较大，因此加速不明显。\n",
      "\n",
      "=== 输出精度对比 ===\n",
      "原始模型 Logits: [[-4.3731527  4.7253337]]\n",
      "量化模型 Logits: [[-4.2938247  4.6516433]]\n",
      "均方误差 (MSE): 0.005862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 1. 加载量化后的模型\n",
    "# 修改点3: 显式指定 file_name，解决 \"Could not find any ONNX files\" 的警告\n",
    "quantized_model_path = \"./quantized_distilbert_sst2\"\n",
    "quantized_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    quantized_model_path, \n",
    "    file_name=\"model_quantized.onnx\"\n",
    ")\n",
    "\n",
    "# 2. 定义辅助函数：获取目录大小\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "original_onnx_path = \"./original_onnx_model\"\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_name, export=False)\n",
    "ort_model.save_pretrained(original_onnx_path)\n",
    "\n",
    "# 3. 对比模型大小\n",
    "original_size = get_dir_size(original_onnx_path) / (1024 * 1024) \n",
    "quantized_size = get_dir_size(quantized_model_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"=== 模型大小对比 ===\")\n",
    "print(f\"原始模型大小 (FP32 ONNX): {original_size:.2f} MB\")\n",
    "print(f\"量化模型大小 (INT8 ONNX): {quantized_size:.2f} MB\")\n",
    "print(f\"模型压缩率: {original_size / quantized_size:.2f}倍\")\n",
    "\n",
    "# 4. 对比推理速度\n",
    "def measure_latency(model, tokenizer, text, n_loops=100):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    for _ in range(10): model(**inputs) # Warmup\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(n_loops):\n",
    "        model(**inputs)\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) / n_loops * 1000\n",
    "\n",
    "text = \"This movie is absolutely wonderful and I loved every moment of it.\"\n",
    "original_latency = measure_latency(ort_model, tokenizer, text)\n",
    "quantized_latency = measure_latency(quantized_model, tokenizer, text)\n",
    "\n",
    "print(f\"\\n=== 推理速度对比 (CPU, Batch Size=1) ===\")\n",
    "print(f\"原始模型平均耗时: {original_latency:.2f} ms\")\n",
    "print(f\"量化模型平均耗时: {quantized_latency:.2f} ms\")\n",
    "print(f\"速度提升: {original_latency / quantized_latency:.2f}倍\")\n",
    "print(\"注意：在 Batch Size=1 时，CPU 计算往往不是瓶颈，Python 调用开销占比较大，因此加速不明显。\")\n",
    "\n",
    "# 5. 对比输出精度\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "original_outputs = ort_model(**inputs).logits.detach().numpy()\n",
    "quantized_outputs = quantized_model(**inputs).logits.detach().numpy()\n",
    "\n",
    "print(f\"\\n=== 输出精度对比 ===\")\n",
    "print(f\"原始模型 Logits: {original_outputs}\")\n",
    "print(f\"量化模型 Logits: {quantized_outputs}\")\n",
    "mse = np.mean((original_outputs - quantized_outputs)**2)\n",
    "print(f\"均方误差 (MSE): {mse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiChunk_w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
